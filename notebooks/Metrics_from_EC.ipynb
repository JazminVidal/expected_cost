{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(\"../\")\n",
    "from expected_cost import ec, other_metrics, utils\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, balanced_accuracy_score\n",
    "# from sklearn.metrics import class_likelihood_ratios no funca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "DATA_PATH=\"outputs/data/\"\n",
    "# Load data\n",
    "df_augmented_logits = pd.read_pickle(f'{DATA_PATH}augmented_logits.pickle')  \n",
    "# System targets and scores to be used for computing metrics\n",
    "targets = df_augmented_logits['labels'].values\n",
    "scores = df_augmented_logits['logit_scores'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for EC and other metrics computed from EC\n",
    "# Number of classes\n",
    "C = 2\n",
    "# Beta parameter for Fscore\n",
    "beta = 1\n",
    "# Prior for class of interest. In this case 1 in PATH.\n",
    "p0 = utils.get_binary_data_priors(targets)[1]\n",
    "# Prior vector with given above and all other priors being equal to (1-p0)/(C-1)\n",
    "data_priors = np.array([p0] + [(1 - p0) / (C - 1)] * (C - 1))\n",
    "# Uniform priors\n",
    "unif_priors = np.ones(C) / C\n",
    "# Usual 0-1 cost matrix\n",
    "costs_01 = ec.cost_matrix.zero_one_costs(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 0.5\n",
    "decisions = np.array([1 if i>thr else 0 for i in scores])\n",
    "# Counts\n",
    "N0, N1, K00, K11, K01, K10 = utils.get_counts_from_binary_data(targets, decisions)\n",
    "K = N0 + N1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68108652 0.31891348] 0.6810865191146881 0.6810865191146881\n",
      "0.0454322862120153\n"
     ]
    }
   ],
   "source": [
    "# Fscore \n",
    "fs1 = 1-other_metrics.f_score(K10, K01, N0, N1)\n",
    "fs2 = other_metrics.one_minus_fscore_from_EC(targets, decisions, beta)\n",
    "fs3 = 1-f1_score(targets, decisions)\n",
    "assert np.round(fs1,2) == np.round(fs2,2) == np.round(fs3,2)\n",
    "print(fs2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = other_metrics.accuracy(K00, K11, K)\n",
    "acc2 = other_metrics.accuracy_from_EC(targets, decisions)\n",
    "acc3 = accuracy_score(targets, decisions)\n",
    "assert np.round(acc1,2) == np.round(acc2,2) == np.round(acc3,2)\n",
    "print(acc2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_acc1 = other_metrics.bal_accuracy(C, K00, K11, N0, N1)\n",
    "bal_acc2 = other_metrics.bal_accuracy_from_EC(targets, decisions)\n",
    "bal_acc3 = balanced_accuracy_score(targets, decisions)\n",
    "assert np.round(bal_acc1,2) == np.round(bal_acc2,2) == np.round(bal_acc3,2)\n",
    "print(bal_acc2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc1 = other_metrics.mc_coeff(K10, K01, N0, N1)\n",
    "mcc2 = other_metrics.mccoeff_from_EC(targets, decisions)\n",
    "mcc3 = matthews_corrcoef(targets, decisions)\n",
    "assert np.round(bal_acc1,2) == np.round(bal_acc2,2) == np.round(bal_acc3,2)\n",
    "print(mcc2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp1 = other_metrics.lr_plus(K10, K01, N0, N1)\n",
    "lrp2 = other_metrics.lrplus_from_EC(targets, decisions)\n",
    "#lrp3 = class_likelihood_ratios(targets, decisions)\n",
    "assert np.round(lrp1,2) == np.round(lrp2,2)\n",
    "print(lrp2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not implemented in sklearn see discussion here:\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/22136\n",
    "nb1 = other_metrics.nb(K01, K11, K, pt=thr)\n",
    "nb2 = other_metrics.nb_from_EC(targets, decisions, pt=thr)\n",
    "nb1, nb2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
