{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(\"../\")\n",
    "from expected_cost import ec, other_metrics, utils\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, balanced_accuracy_score\n",
    "# from sklearn.metrics import class_likelihood_ratios not working"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "DATA_PATH=\"outputs/data/\"\n",
    "# Load data\n",
    "df_augmented_logits = pd.read_pickle(f'{DATA_PATH}augmented_logits.pickle')  \n",
    "# System targets and scores to be used for computing metrics\n",
    "targets = df_augmented_logits['labels'].values\n",
    "scores = df_augmented_logits['logit_scores'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for EC and other metrics computed from EC\n",
    "# Number of classes\n",
    "C = 2\n",
    "# Beta parameter for Fscore\n",
    "beta = 1\n",
    "# Prior for class of interest. In this case 1\n",
    "p0 = utils.get_binary_data_priors(targets)[1]\n",
    "# Prior vector with given above and all other priors being equal to (1-p0)/(C-1)\n",
    "data_priors = np.array([p0] + [(1 - p0) / (C - 1)] * (C - 1))\n",
    "# Uniform priors\n",
    "unif_priors = np.ones(C) / C\n",
    "# Usual 0-1 cost matrix\n",
    "costs_01 = ec.cost_matrix.zero_one_costs(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 0.5\n",
    "decisions = np.array([1 if i>thr else 0 for i in scores])\n",
    "# Counts\n",
    "N0, N1, K00, K11, K01, K10 = utils.get_counts_from_binary_data(targets, decisions)\n",
    "K = N0 + N1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-Fscore\n",
    "\n",
    "The F-beta score, which we will call F-beta, is defined as 1 minus the effectiveness. It assumes that the problem\n",
    "is not symmetric and one of the classes is taken as the class of interest to be detected. \n",
    "\n",
    "F-beta takes values between 0 and 1. Larger values indicate better performance, contrary to the EC for\n",
    "which larger values indicate worse performance. For this reason, in order to compare F-beta with EC, it\n",
    "is convenient to work with 1 âˆ’ F-beta. \n",
    "\n",
    "1-F-beta is the EC for the binary case when setting: \n",
    "- The priors to be those found on the evaluation data. \n",
    "- The costs are given by c11 = c22 = 0, c12 = 1, c21 = beta**2,\n",
    "And is then divided by a scaling factor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68108652 0.31891348] 0.6810865191146881 0.6810865191146881\n",
      "0.0454322862120153\n"
     ]
    }
   ],
   "source": [
    "# Fscore \n",
    "fs1 = 1-other_metrics.f_score(K10, K01, N0, N1)\n",
    "fs2 = other_metrics.one_minus_fscore_from_EC(targets, decisions, beta)\n",
    "fs3 = 1-f1_score(targets, decisions)\n",
    "assert np.round(fs1,2) == np.round(fs2,2) == np.round(fs3,2)\n",
    "print(fs2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "It is defined as the fraction of the samples for which the system decided the correct class. It is 1-Error Rate (the fraction of the samples incorrectly detected). The Error rate is the EC when: \n",
    "- The priors are set to be those found in the evaluation data\n",
    "- The cost matrix is the usual square 0-1 cost matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = other_metrics.accuracy(K00, K11, K)\n",
    "acc2 = other_metrics.accuracy_from_EC(targets, decisions)\n",
    "acc3 = accuracy_score(targets, decisions)\n",
    "assert np.round(acc1,2) == np.round(acc2,2) == np.round(acc3,2)\n",
    "print(acc2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced accuracy\n",
    "\n",
    "Used when the errors in the detection of the minority classes are considered more severe than those in the majority classes. \n",
    "The metric is defined as the average of the recall values  (the fraction of samples from a certain class correctly labelled) over all classes. It can be seen as a special case of EC when: \n",
    "- The priors are set to be those found in the evaluation data. \n",
    "- The cost matrix is c_ij = 1/(CP_i)\n",
    "\n",
    "With C the number of classes and P_i the priors for class i. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_acc1 = other_metrics.bal_accuracy(C, K00, K11, N0, N1)\n",
    "bal_acc2 = other_metrics.bal_accuracy_from_EC(targets, decisions)\n",
    "bal_acc3 = balanced_accuracy_score(targets, decisions)\n",
    "assert np.round(bal_acc1,2) == np.round(bal_acc2,2) == np.round(bal_acc3,2)\n",
    "print(bal_acc2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc1 = other_metrics.mc_coeff(K10, K01, N0, N1)\n",
    "mcc2 = other_metrics.mccoeff_from_EC(targets, decisions)\n",
    "mcc3 = matthews_corrcoef(targets, decisions)\n",
    "assert np.round(bal_acc1,2) == np.round(bal_acc2,2) == np.round(bal_acc3,2)\n",
    "print(mcc2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp1 = other_metrics.lr_plus(K10, K01, N0, N1)\n",
    "lrp2 = other_metrics.lrplus_from_EC(targets, decisions)\n",
    "#lrp3 = class_likelihood_ratios(targets, decisions)\n",
    "assert np.round(lrp1,2) == np.round(lrp2,2)\n",
    "print(lrp2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net benefit\n",
    "\n",
    "Is a metric used in binary classification for some medical applications. It can be implemented in terms of NEC by setting: \n",
    "- The priors are set to be those found in the evaluation data. \n",
    "- A cost matrix with c11=c22=0, c21=1 and c12=pt/(1-pt) with pt the value for the decision threshold. \n",
    "\n",
    "And then multiplying NEC to a constant and substractig the result from P2 (the priors for class 2). \n",
    "\n",
    "Note that NB looses one of the most attractive qualities of NEC: that\n",
    "a value of 1.0 indicates a system that has the same performance as a naive system that does not use\n",
    "the input samples to make its decisions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb1 = other_metrics.nb(K01, K11, K, pt=thr)\n",
    "nb2 = other_metrics.nb_from_EC(targets, decisions, pt=thr)\n",
    "nb1, nb2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
